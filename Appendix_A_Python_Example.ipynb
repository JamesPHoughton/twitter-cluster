{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Cluster Identification and Transition Analysis in Python\n",
    "\n",
    "This code takes messages that are on twitter, and extracts their hashtags. It then constructs a set of weighted and unweighted network structures based upon co-citation of hashtags within a tweet. The network diagrams are interpreted to have a set of clusters within them which represent 'conversations' that are happening in the pool of twitter messages. We track similarity between clusters from day to day to investigate how conversations develop.\n",
    "\n",
    "### Utilities\n",
    "These scripts depend upon a number of external utilities as listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "import glob\n",
    "import dateutil.parser\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'cos-parallel': u'tools/cosparallel-0.99/./cos',\n",
       " u'data_dir': u'../data/',\n",
       " u'maximal_cliques': u'tools/cosparallel-0.99/extras/./maximal_cliques',\n",
       " u'python_working_dir': u'../PYTHON/working/',\n",
       " u'unicage_working_dir': u'../UNICAGE/working/'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the locations of the various elements of the analysis\n",
    "with open('config.json','r') as jfile:\n",
    "    config = json.load(jfile)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Files\n",
    "We have twitter messages saved as compressed files, where each line in the file is the JSON object that the twitter sample stream returns to us. The files are created by splitting the streaming dataset according to a fixed number of lines - not necessarily by a fixed time or date range. A description of the collection process can be found in Appendix D.\n",
    "\n",
    "All the files have the format `posts_sample_YYYYMMDD_HHMMSS_aa.txt` where the date listed is the date at which the stream was initialized. Multiple days worth of stream may be grouped under the same second, as long as the stream remains unbroken. If we have to restart the stream, then a new datetime will be added to the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with 5665 input files\n"
     ]
    }
   ],
   "source": [
    "# Collect a list of all the filenames that will be working with\n",
    "files = glob.glob(config['data_dir']+'posts_sample*.gz')\n",
    "print 'working with %i input files'%len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Structures\n",
    "Its helpful to have a list of the dates in the range that we'll be looking at, because we can't always just add one to get to the next date. Here we create a list of strings with dates in the format 'YYYYMMDD'. The resulting list looks like:\n",
    "\n",
    "    ['20141101', '20141102', '20141103', ... '20150629', '20150630']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investigating 242 dates\n"
     ]
    }
   ],
   "source": [
    "dt = datetime.datetime(2014, 11, 1)\n",
    "end = datetime.datetime(2015, 7, 1)\n",
    "step = datetime.timedelta(days=1)\n",
    "\n",
    "dates = []\n",
    "while dt < end:\n",
    "    dates.append(dt.strftime('%Y%m%d'))\n",
    "    dt += step\n",
    "    \n",
    "print 'investigating %i dates'%len(dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 1: Count hashtag pairs\n",
    "The most data-intensive part of the analysis is this first piece, which parses all of the input files, and counts the various combinations of hashtags on each day.\n",
    "\n",
    "In this demonstration we perform this counting in memory, which is sufficient for date ranges on the order of weeks, but becomes unwieldy beyond this timescale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10 20 30 40 50 60 70 80 "
     ]
    }
   ],
   "source": [
    "#construct a counter object for each date\n",
    "tallydict = dict([(date, Counter()) for date in dates])\n",
    "\n",
    "#iterate through each of the input files in the date range\n",
    "for i, zfile in enumerate(files):\n",
    "    if i%10 == 0: #save every 10 files\n",
    "        print i,\n",
    "        with open(config['python_working_dir']+\"tallydict.pickle\", \"wb\" ) as picklefile:\n",
    "            pickle.dump(tallydict, picklefile)  \n",
    "        with open(config['python_working_dir']+\"progress.txt\", 'a') as progressfile:    \n",
    "            progressfile.write(str(i)+': '+zfile+'\\n')\n",
    "            \n",
    "    try:\n",
    "        with gzip.open(zfile) as gzf:\n",
    "            #look at each line in the file\n",
    "            for line in gzf:\n",
    "                try:\n",
    "                    #parse the json object \n",
    "                    parsed_json = json.loads(line)\n",
    "                    # we only want to look at tweets that are in \n",
    "                    # english, so check that this is the case.\n",
    "                    if parsed_json.has_key('lang'):\n",
    "                        if parsed_json['lang'] =='en':\n",
    "                            #look only at messages with more than two hashtags, \n",
    "                            #as these are the only ones that make connections\n",
    "                            if len(parsed_json['entities']['hashtags']) >=2:\n",
    "                                #extract the hashtags to a list \n",
    "                                taglist = [entry['text'].lower() for entry in \n",
    "                                           parsed_json['entities']['hashtags']]\n",
    "                                # identify the date in the message \n",
    "                                # this is important because sometimes messages \n",
    "                                # come out of order.\n",
    "                                date = dateutil.parser.parse(parsed_json['created_at'])\n",
    "                                date = date.strftime(\"%Y%m%d\") \n",
    "                                #look at all the combinations of hashtags in the set\n",
    "                                for pair in combinations(taglist, 2):\n",
    "                                    #count up the number of alpha sorted tag pairs\n",
    "                                    tallydict[date][' '.join(sorted(pair))] += 1\n",
    "                except: #error reading the line\n",
    "                    print 'd',\n",
    "    except: #error reading the file\n",
    "        print 'error in', zfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the counter object periodically in case of a serious error. If we have one, we can load what we've already accomplished with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(config['python_working_dir']+\"tallydict.pickle\", \"r\" ) as picklefile:\n",
    "    tallydict = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 2: Create Weighted Edge Lists\n",
    "Having created this sorted set of tag pairs, we should write these counts to files. We'll create one file for each day. The files themselves will have one pair of words followed by the number of times those hashtags were spotted in combination on each day. For Example:\n",
    "\n",
    "```\n",
    "PCMS champs 3\n",
    "TeamFairyRose TeamFollowBack 3\n",
    "instadaily latepost 2\n",
    "LifeGoals happy 2\n",
    "DanielaPadillaHoopsForHope TeamBiogesic 2\n",
    "shoes shopping 5\n",
    "kordon saatc 3\n",
    "DID Leg 3\n",
    "entrepreneur grow 11\n",
    "Authors Spangaloo 2\n",
    "```\n",
    "\n",
    "We'll save these in a very specific directory structure that will simplify keeping track of our data down the road, when we want to do more complex things with it. An example:\n",
    "\n",
    "```\n",
    "twitter/\n",
    "   20141116/\n",
    "       weighted_edges_20141116.txt\n",
    "   20141117/\n",
    "       weighted_edges_20141117.txt\n",
    "   20141118/\n",
    "       weighted_edges_20141118.txt\n",
    "   etc...\n",
    "```\n",
    "\n",
    "We create a row for every combination that has a count of at least two.\n",
    "\n",
    "In this code we'll use some of the iPython 'magic' functions for file manipulation, which let us execute shell commands as if through a terminal. Lines prepended with the exclaimation point `!` will get passed to the shell. We can include python variables in the command by prepending them with a dollar sign `$`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for key in tallydict.keys(): #keys are datestamps\n",
    "    #create a directory for the date in question\n",
    "    date_dir = config['python_working_dir']+key\n",
    "    if not os.path.exists(date_dir):\n",
    "        os.makedirs(date_dir)\n",
    "    #replace old file, instead of append\n",
    "    with open(config['python_working_dir']+key+'/weighted_edges_'+key+'.txt', 'w') as fout: \n",
    "        for item in tallydict[key].iteritems():\n",
    "            if item[1] >= 2: #throw out the ones that only have one edge\n",
    "                fout.write(item[0].encode('utf8')+' '+str(item[1])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get a list of the wieghed edgelist files, which will be helpful later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 242 weighted edgelist files\n"
     ]
    }
   ],
   "source": [
    "weighted_files = glob.glob(config['python_working_dir']+'*/weight*.txt')\n",
    "print 'created %i weighted edgelist files'%len(weighted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 3: Construct unweigheted edgelist\n",
    "\n",
    "We make an unweighted list of edges by throwing out everything below a certain threshold. We'll do this for a range of different thresholds, so that we can compare the results later. Looks like:\n",
    "\n",
    "```\n",
    "FoxNflSunday tvtag\n",
    "android free\n",
    "AZCardinals Lions\n",
    "usa xxx\n",
    "كبلز متحرره\n",
    "CAORU TEAMANGELS\n",
    "RT win\n",
    "FarCry4 Games\n",
    "```\n",
    "\n",
    "We do this for thresholds between 2 and 15 (for now, although we may want to change later) so the directory structure looks like:\n",
    "```\n",
    "twitter/\n",
    "   20141116/\n",
    "       th_02/\n",
    "           unweighted_20141116_th_02.txt\n",
    "       th_03/\n",
    "           unweighted_20141116_th_03.txt\n",
    "       th_04/\n",
    "           unweighted_20141116_th_04.txt\n",
    "       etc...\n",
    "   20151117/\n",
    "       th_02/\n",
    "           unweighted_20141117_th_02.txt\n",
    "       etc...\n",
    "   etc...\n",
    "```\n",
    "\n",
    "Filenames include the date and the threshold, and the fact that these files are unweighted edge lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for threshold in range (2, 15):\n",
    "    for infile_name in weighted_files:\n",
    "        date_dir = os.path.dirname(infile_name)\n",
    "        date = date_dir.split('/')[-1]\n",
    "        weighted_edgefile = os.path.basename(infile_name)\n",
    "        \n",
    "        #create a subdirectory for each threshold we choose\n",
    "        th_dir = date_dir+'/th_%02i'%threshold\n",
    "        if not os.path.exists(th_dir):\n",
    "            os.makedirs(th_dir)\n",
    "        \n",
    "        # load the weighted edgelists file and filter it to \n",
    "        # only include values above the threshold\n",
    "        df = pd.read_csv(infile_name, sep=' ', header=None, \n",
    "                         names=['Tag1', 'Tag2', 'count'])\n",
    "        filtered = df[df['count']>threshold][['Tag1','Tag2']]\n",
    "        \n",
    "        #write out an unweighted edgelist file for each threshold\n",
    "        outfile_name = th_dir+'/unweighted_'+date+'_th_%02i'%threshold+'.txt'\n",
    "        with open(outfile_name, 'w') as fout: #replace old file, instead of append\n",
    "            for index, row in filtered.iterrows():\n",
    "                try:\n",
    "                    fout.write(row['Tag1']+' '+row['Tag2']+'\\n')\n",
    "                except:\n",
    "                    print 'b',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get a list of all the unweighted edgelist files we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 3146 unweighted edgelist files\n"
     ]
    }
   ],
   "source": [
    "unweighted_files = glob.glob(config['python_working_dir']+'*/*/unweight*.txt')\n",
    "print 'created %i unweighted edgelist files'%len(unweighted_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Step 4: Find the communities\n",
    "\n",
    "We're using [COS Parallel](http://sourceforge.net/p/cosparallel/wiki/Home/) to identify k-cliques, so we feed each unweighted edge file into the `./maximal_cliques` preprocessor, and then the `./cos` algorithm. \n",
    "\n",
    "The unweighed edgelist files should be in the correct format for `./maximal_cliques` to process at this point. \n",
    "\n",
    "`./maximal_cliques` translates each node name into an integer to make it faster and easier to deal with, and so the output from this file is both a listing of all of the maximal cliques in the network, with an extension `.mcliques`, and a mapping of all of the integer nodenames back to the original text names, having extension `.map`.\n",
    "\n",
    "It is a relatively simple task to feed each unweighed edgelist we generated above into the `./maximal_cliques` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for infile in unweighted_files:\n",
    "    th_dir = os.path.dirname(infile)\n",
    "    th_file = os.path.basename(infile)\n",
    "    #operate the command in the directory where we want the files created\n",
    "    subprocess.call([os.getcwd()+'/'+config['maximal_cliques'], th_file], cwd=th_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this step is complete, we then feed the `.mcliques` output files into the `cosparallel` algorith."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'../PYTHON/working/20150502/th_05/unweighted_20150502_th_05.txt.mcliques',\n",
       " u'../PYTHON/working/20150502/th_03/unweighted_20150502_th_03.txt.mcliques',\n",
       " u'../PYTHON/working/20150502/th_02/unweighted_20150502_th_02.txt.mcliques']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxclique_files = glob.glob(config['python_working_dir']+'*/*/*.mcliques')\n",
    "print 'created %i maxcliques files'%len(maxclique_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "for infile in maxclique_files:\n",
    "    mc_dir = os.path.dirname(infile)\n",
    "    mc_file = os.path.basename(infile)\n",
    "    subprocess.call([os.getcwd()+'/'+config['cos-parallel'], mc_file], cwd=mc_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Translate back from numbers to actual words\n",
    "The algorithms we just ran abstract away from the actual text words and give us a result with integer collections and a map back to the original text. So we apply the map to recover the clusters in terms of their original words, and give each cluster a unique identifier:\n",
    "\n",
    "```\n",
    "0 Ferguson Anonymous HoodsOff OpKKK\n",
    "1 Beauty Deals Skin Hair \n",
    "2 Family gym sauna selfie\n",
    "etc...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'../PYTHON/working/20150502/th_05/4_communities.txt',\n",
       " u'../PYTHON/working/20150502/th_05/10_communities.txt',\n",
       " u'../PYTHON/working/20150502/th_05/6_communities.txt']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "community_files = glob.glob(config['python_working_dir']+'*/*/[0-9]*communities.txt')\n",
    "print 'created %i community files':%len(community_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we'll be reading a lot of files like this, \n",
    "# so it makes sense to create a function to help with it.\n",
    "def read_cluster_file(infile_name):\n",
    "    \"\"\" take a file output from COS and return a dictionary\n",
    "    with keys being the integer cluster name, and \n",
    "    elements being a set of the keywords in that cluster\"\"\"\n",
    "    clusters = dict()\n",
    "    with open(infile_name, 'r') as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            #the name of the cluster is the bit before the colon\n",
    "            name = line.split(':')[0] \n",
    "            if not clusters.has_key(name):\n",
    "                clusters[name] = set()\n",
    "            #the elements of the cluster are after the colon, space delimited\n",
    "            nodes = line.split(':')[1].split(' ')[:-1] \n",
    "            for node in nodes:\n",
    "                clusters[name].add(int(node))\n",
    "    return clusters        \n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "for infile in community_files:\n",
    "    c_dir = os.path.dirname(infile)\n",
    "    c_file = os.path.basename(infile)\n",
    "    \n",
    "    #load the map into a pandas series to make it easy to translate\n",
    "    map_filename = glob.glob('%s/*.map'%c_dir)\n",
    "    mapping = pd.read_csv(map_filename[0], sep=' ', header=None, \n",
    "                          names=['word', 'number'], index_col='number')\n",
    "\n",
    "    clusters = read_cluster_file(infile)\n",
    "    #create a named cluster file in the same directory\n",
    "    with open(c_dir+'/named'+c_file, 'w') as fout:\n",
    "        for name, nodes in clusters.iteritems():\n",
    "            fout.write(' '.join([str(name)]+\n",
    "                                [mapping.loc[int(node)]['word'] for node in list(nodes)]+\n",
    "                                ['\\n']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, we'll write a function to read the files we're creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_named_cluster_file(infile_name):\n",
    "    \"\"\" take a file output from COS and return a \"\"\"\n",
    "    clusters = dict()\n",
    "    with open(infile_name, 'r') as fin:\n",
    "        for i, line in enumerate(fin):\n",
    "            name = line.split(' ')[0]\n",
    "            if not clusters.has_key(name):\n",
    "                clusters[int(name)] = set()\n",
    "            nodes = line.split(' ')[1:-1]\n",
    "            for node in nodes:\n",
    "                clusters[int(name)].add(node)\n",
    "    return clusters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compute transition likelihoods\n",
    "\n",
    "We want to know how a cluster on one day is related to a cluster on the next day. For now, we'll use a brute-force algorithm of counting the number of nodes in a cluster that are present in each of the subsequent day's cluster. From this we can get a likelihood of sorts for subsequent clusters.\n",
    "\n",
    "We'll define a function that, given the clusers on day 1 and day 2, creates a matrix from the two, with day1 clusters as row elements and day2 clusters as column elements. The entries to the matrix are the number of nodes shared by each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#brute force, without the intra-day clustering\n",
    "def compute_transition_likelihood(current_clusters, next_clusters):\n",
    "    transition_likelihood = np.empty([max(current_clusters.keys())+1, \n",
    "                                      max(next_clusters.keys())+1])\n",
    "    for current_cluster, current_elements in current_clusters.iteritems():\n",
    "        for next_cluster, next_elements in next_clusters.iteritems():\n",
    "            #the size of the intersection of the sets\n",
    "            transition_likelihood[current_cluster, next_cluster] = (\n",
    "                           len(current_elements & next_elements) ) \n",
    "    return transition_likelihood\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute transition matricies for all clusters with every k and every threshold. We'll save the matrix for transitioning from Day1 to Day2 in Day1's folder. In many cases, there won't be an appropriate date/threshold/k combination, so we'll just skip that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this should compute and store all of the transition likelihoods\n",
    "\n",
    "for current_date in dates[:-1]:\n",
    "    next_date = dates[dates.index(current_date)+1]\n",
    "    for threshold in range(2,15):\n",
    "        for k in range(3, 20):\n",
    "\n",
    "            current_file_name = config['python_working_dir']+'%s/th_%02i/named%i_communities.txt'%(current_date, \n",
    "                                                                      threshold, k)\n",
    "            \n",
    "            next_file_name = config['python_working_dir']+'%s/th_%02i/named%i_communities.txt'%(next_date, \n",
    "                                                                   threshold, k)\n",
    "            \n",
    "            if os.path.isfile(current_file_name) & os.path.isfile(next_file_name): \n",
    "                current_clusters = read_named_cluster_file(current_file_name)\n",
    "                next_clusters = read_named_cluster_file(next_file_name)                           \n",
    "            \n",
    "                transition = compute_transition_likelihood(current_clusters, \n",
    "                                                           next_clusters)\n",
    "                transitiondf = pd.DataFrame(data=transition, \n",
    "                                            index=current_clusters.keys(),\n",
    "                                            columns=next_clusters.keys())\n",
    "\n",
    "                transitiondf.to_csv(current_file_name[:-4]+'_transition.csv')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 15608 transition matrix files\n"
     ]
    }
   ],
   "source": [
    "transition_files = glob.glob(config['python_working_dir']+'*/*/named*_communities_transition.csv')\n",
    "print 'created %i transition matrix files'%len(transition_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
