{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Cluster Identification and Transition Analysis in Unicage\n",
    "\n",
    "This code replicates the functionality found in appendix A one step at a time, using shell programming and the Unicage development platform. Each of the scripts listed here is found at https://github.com/JamesPHoughton/twitter-cluster/tree/master/unicage. \n",
    "\n",
    "\n",
    "\n",
    "#### Running these scripts\n",
    "This analysys process is separated to 7 steps. You can run each or all steps using the helper script `twitter_analysis.sh` as follows:\n",
    "\n",
    "```shell\n",
    "$ twitter_analysis.sh <start_step_no> <end_step_no>\n",
    "```\n",
    "A key to the step numbers is:\n",
    "```\n",
    "      1 - list_word_pairings.sh\n",
    "      2 - wgted_edge_gen.sh\n",
    "      3 - unwgted_edge_gen.sh\n",
    "      4 - run_mcliques.sh\n",
    "      5 - run_cos.sh\n",
    "      6 - back_to_org_words.sh\n",
    "      7 - compute_transition_likelihoods.sh\n",
    "```\n",
    "\n",
    "For example, to execute step4 to step6:\n",
    "```shell\n",
    "$ twitter_analysis.sh 4 6\n",
    "```\n",
    "\n",
    "To execute step2 only:\n",
    "```shell\n",
    "$ twitter_analysis.sh 2 2\n",
    "```\n",
    "\n",
    "To execute all steps:\n",
    "```shell\n",
    "$ twitter_analysis.sh 1 7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: list_word_pairings.sh\n",
    "This script creates lists of hashtag pairs from json files.\n",
    "\n",
    "Output: produces `DATA/result.XXXX.`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash\n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "mkdir -p ${datad}\n",
    "\n",
    "n=0\n",
    "\n",
    "# Process zipped files/\n",
    "echo ${rawd}/posts_sample*.gz                                            |\n",
    "tarr                                                                     |\n",
    "while read zipfile; do         \n",
    "  n=$((n+1))\n",
    "  echo $zipfile $n\n",
    "\n",
    "  {\n",
    "    zcat $zipfile                                                        |\n",
    "    jq -c '{time: .timestamp_ms, hashtag: [.entities.hashtags[]?.text]}' |\n",
    "    grep \"time\"                                                          |\n",
    "    grep \"hashtag\"                                                       |\n",
    "    grep -v ':null'                                                      |\n",
    "    tr -d '{}[] '                                                        |\n",
    "    tr ':' ','                                                           |\n",
    "    fromcsv                                                              |\n",
    "    # 1: \"time\" 2: timestamp (epoch msec) 3: \"hashtag\" 4-N: hashtags\n",
    "\n",
    "    awk 'NF>5{for(i=4;i<=NF;i++)for(j=i+1;j<=NF;j++){print $i,$j,int($2/1000)}}' |\n",
    "    # list all possible 2 word combinations with timestamp. 1: word1 2: word2 3: timestamp (epoch sec)\n",
    "\n",
    "    calclock -r 3                                                        |\n",
    "    # 1: word1 2: word2 3: timestamp (epoch sec) 4: timestamp (YYYYMMDDhhmmss)\n",
    "\n",
    "    self 1 2 4.1.8                                                       |\n",
    "    # 1: word1 2: word2 3: timestamp (YYYYMMDD)\n",
    "\n",
    "    msort key=1/3                                                        | \n",
    "    count 1 3                                                            > ${datad}/result.$n\n",
    "    # count lines having the same word combination and timestamp 1:word1 2:word2 3:date 4:count\n",
    "\n",
    "    # run 5 processes in parallel\n",
    "    touch ${semd}/sem.$n\n",
    "   } &\n",
    "   if [ $((n % 5)) -eq 0 ]; then\n",
    "     eval semwait ${semd}/sem.{$((n-4))..$n}\n",
    "     eval rm ${semd}/sem.*\n",
    "   fi\n",
    "done\n",
    "\n",
    "wait\n",
    "\n",
    "n=$(ls ${datad}/result.* | sed -e 's/\\./ /g' | self NF | msort key=1n | tail -1)\n",
    "\n",
    "# Process unzipped files.\n",
    "#    *There are unzipped files in raw data dir(/home/James.P.H/data).\n",
    "echo ${rawd}/posts_sample*\t\t\t\t\t\t|\n",
    "tarr\t\t\t\t\t\t\t\t\t        |\n",
    "self 1 1.-3.3\t\t\t\t\t\t\t     \t|\n",
    "delr 2 '.gz'\t\t\t\t\t\t\t\t    |\n",
    "self 1\t\t\t\t\t\t\t\t\t        |\n",
    "while read nozipfile; do         \n",
    "  n=$((n+1))\n",
    "  echo $nozipfile $n\n",
    "\n",
    "  {\n",
    "    cat $nozipfile                                                       |\n",
    "    jq -c '{time: .timestamp_ms, hashtag: [.entities.hashtags[]?.text]}' |\n",
    "    grep \"time\"                                                          |\n",
    "    grep \"hashtag\"                                                       |\n",
    "    grep -v ':null'                                                      |\n",
    "    tr -d '{}[] '                                                        |\n",
    "    tr ':' ','                                                           |\n",
    "    fromcsv                                                              |\n",
    "    # 1: \"time\" 2: timestamp (epoch msec) 3: \"hashtag\" 4-N: hashtags\n",
    "\n",
    "    awk 'NF>5{for(i=4;i<=NF;i++)for(j=i+1;j<=NF;j++){print $i,$j,int($2/1000)}}' |\n",
    "    # list all possible 2 word combinations with timestamp. 1: word1 2: word2 3: timestamp (epoch sec)\n",
    "\n",
    "    calclock -r 3                                                        |\n",
    "    # 1: word1 2: word2 3: timestamp (epoch sec) 4: timestamp (YYYYMMDDhhmmss)\n",
    "\n",
    "    self 1 2 4.1.8                                                       |\n",
    "    # 1: word1 2: word2 3: timestamp (YYYYMMDD)\n",
    "\n",
    "    msort key=1/3                                                        | \n",
    "    count 1 3                                                            > ${datad}/result.$n\n",
    "    # count lines having the same word combination and timestamp 1:word1 2:word2 3:date 4:count\n",
    "\n",
    "    # run 5 processes in parallel\n",
    "    touch ${semd}/sem.$n\n",
    "   } &\n",
    "   if [ $((n % 5)) -eq 0 ]; then\n",
    "     eval semwait ${semd}/sem.{$((n-4))..$n}\n",
    "     eval rm ${semd}/sem.*\n",
    "   fi\n",
    "done\n",
    "\n",
    "#semwait \"${semd}/sem.*\"\n",
    "wait\n",
    "eval rm ${semd}/sem.*\n",
    "\n",
    "exit 0\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: wgted_edge_gen.sh\n",
    "This script creates weighted edgelists from `result.*` and places them under yyyymmdd dirs. \n",
    "\n",
    "Output: produces `twitter/yyyymmdd/weighted_edges_yyyymmdd.txt`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash -xv\n",
    "\n",
    "# wgted_edge_gen.sh creates weighted edgelists from result.*\n",
    "# and place them under yyyymmdd dirs.\n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=/${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "# TODO debug\n",
    "#datad=${homed}/DATA.mini\n",
    "#workd=${homed}/twitter.mini\n",
    "\n",
    "\n",
    "tmp=/tmp/$$\n",
    "\n",
    "# error function: show ERROR and exit with 1\n",
    "ERROR_EXIT() {\n",
    "  echo \"ERROR\"\n",
    "  exit 1\n",
    "}\n",
    "\n",
    "mkdir -p ${workd}\n",
    "\n",
    "# count the number of files\n",
    "n=$(ls ${datad}/result.* | gyo)\n",
    "\n",
    "for i in $(seq 1 ${n} | tarr)\n",
    "do \n",
    "    # 1:Tag1 2:Tag2 3:date 4:count \n",
    "    sorter -d ${tmp}-weighted_edges_%3_${i} ${datad}/result.${i}\n",
    "    [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "done\n",
    "\n",
    "# listup target dates\n",
    "#   /tmp/$$-weighted_edges_YYYYMMDD_i\n",
    "echo ${tmp}-weighted_edges_????????_*\t|\n",
    "tarr\t\t\t\t\t                |\n",
    "ugrep -v '\\?'\t\t\t\t            |\n",
    "sed -e 's/_/ /g'\t\t\t            |\n",
    "self NF-1\t\t\t\t                |\n",
    "msort key=1\t\t\t\t                |\n",
    "uniq\t\t\t\t\t> ${tmp}-datelist\n",
    "# 1:date(YYYYMMDD)\n",
    "\n",
    "[ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "for day in $(cat ${tmp}-datelist); do\n",
    "  mkdir -p ${workd}/${day}\n",
    "\n",
    "  cat ${tmp}-weighted_edges_${day}_*\t|\n",
    "  # 1:word1 2:word2 3:count\n",
    "  msort key=1/2\t\t\t\t|\n",
    "  sm2 1 2 3 3 \t\t\t\t> ${workd}/${day}/weighted_edges_${day}.txt\n",
    "  # 1:word1 2:word2 3:count\n",
    "\n",
    "  [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "done\n",
    "\n",
    "rm ${tmp}-*\n",
    "\n",
    "exit 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: unwgted_edge_gen.sh\n",
    "This script creates unweighted edgelists under the same dir sorted by threshold dirs.\n",
    "\n",
    "Output: produces `twitter/yyyymmdd/th_XX/unweighted_yyyymmdd_th_XX.txt`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash -xv\n",
    "\n",
    "# unwgted_edge_gen.sh expects weighted edgelists \n",
    "# (weighted_edges_yyyymmdd.txt) located in\n",
    "# /home/James.P.H/UNICAGE/twitter/yyyymmdd\n",
    "# and creates unweighted edgelists under the same dir\n",
    "# sorted by threshold dirs. \n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "# TODO test\n",
    "#datad=${homed}/DATA.mini\n",
    "#workd=${homed}/twitter.mini\n",
    "\n",
    "tmp=/tmp/$$\n",
    "\n",
    "# error function: show ERROR and delete tmp files\n",
    "ERROR_EXIT() {\n",
    "  echo \"ERROR\"\n",
    "  rm -f $tmp-*\n",
    "  exit 1\n",
    "}\n",
    "\n",
    "# setting threshold \n",
    "seq 2 15 | maezero 1.2                         > $tmp-threshold\n",
    "[ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "# creating header file\n",
    "itouch \"Hashtag1 Hashtag2 count\" $tmp-header\n",
    "[ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "# create list for all pairs of thresholds and filenames\n",
    "echo ${workd}/201[45]*/weighted_edges_*.txt          |\n",
    "tarr                                                 |\n",
    "joinx $tmp-threshold -                               |\n",
    "# 1:threshold 2:filename\n",
    "while read th wgtedges ; do\n",
    "   echo ${wgtedges}\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "   # define year-month-date variable for dir and file name\n",
    "   yyyymmdd=$(echo ${wgtedges} | awk -F \\/ '{print $(NF-1)}')\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "   \n",
    "   echo ${yyyymmdd} th_${th}\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "   # create threshold dirs under twitter/YYYYMMDD\n",
    "   mkdir -p $(dirname ${wgtedges})/th_${th}\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "   cat $tmp-header ${wgtedges}                       |\n",
    "   # output lines whose count feild is above thresholds\n",
    "   ${toold}/tagcond '%count > '\"${th}\"''             | \n",
    "   # remove threshold feild\n",
    "   tagself Hashtag1 Hashtag2                         |\n",
    "   # remove header\n",
    "   tail -n +2 > ${workd}/${yyyymmdd}/th_${th}/unweighted_${yyyymmdd}_th_${th}.txt\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "done\n",
    "[ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "# delete tmp files\n",
    "rm -f $tmp-*\n",
    "\n",
    "exit 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: run_mcliques.sh\n",
    "This script executes maximal_cliques to all unweigthed edges.\n",
    "\n",
    "Output: produces \n",
    " - `twitter/yyyymmdd/th_XX/unweighted_edges_yyyymmdd_th_XX.txt.map`\n",
    " - `twitter/yyyymmdd/th_XX/unweighted_edges_yyyymmdd_th_XX.txt.mcliques`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash -xv\n",
    "\n",
    "# run_mcliques.sh executes maximal_cliques to all unweigthed edges.\n",
    "# produce unweighted_edges_yyyymmdd.txt.map and unweighted_edges_yyyymmdd.txt.mcliques\n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "# TODO test\n",
    "#datad=${homed}/DATA.mini\n",
    "#workd=${homed}/twitter.mini\n",
    "\n",
    "# error function: show ERROR\n",
    "ERROR_EXIT() {\n",
    "echo \"ERROR\"\n",
    "exit 1\n",
    "}\n",
    "\n",
    "# 共有ライブラリへパスを通す(maximal_cliques用)\n",
    "LD_LIBRARY_PATH=/usr/local/lib:/usr/lib\n",
    "export LD_LIBRARY_PATH\n",
    "\n",
    "\n",
    "# running maximal_cliques\n",
    "for unwgted_edges in ${workd}/*/th_*/unweighted_*_th_*.txt\n",
    "do\n",
    "    echo \"Processing ${unwgted_edges}.\"\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "    \n",
    "    # skip empty files\n",
    "    if [ ! -s ${unwgted_edges} ] ; then\n",
    "        echo \"Skipped $(basename ${unwgted_edges}).\"\n",
    "        continue\n",
    "    fi \n",
    "\n",
    "    cd $(dirname ${unwgted_edges})\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "    ${toold}/maximal_cliques ${unwgted_edges}\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "    # unweighted_edges_yyyymmdd.txt.map (1:Tag 2:integer)\n",
    "    # unweighted_edges_yyyymmdd.txt.mcliques (1...N: integer for nodes N+1: virtual node -1)\n",
    "\n",
    "    echo \"${unwgted_edges} done.\"\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "done\n",
    "\n",
    "exit 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: run_cos.sh\n",
    "This script executes `cos` using `*.mcliques` files to create communities.\n",
    "\n",
    "Output: produces `twitter/yyyymmdd/th_XX/N_communities.txt`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash -xv\n",
    "\n",
    "# run_cos.sh creates communities using *.mcliques files.\n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "# error function: show ERROR\n",
    "ERROR_EXIT() {\n",
    "echo \"ERROR\"\n",
    "exit 1\n",
    "}\n",
    "\n",
    "# 共有ライブラリへパスを通す(cos用)\n",
    "LD_LIBRARY_PATH=/usr/local/lib:/usr/lib\n",
    "export LD_LIBRARY_PATH\n",
    "\n",
    "# running cos\n",
    "for mcliques in ${workd}/*/th_*/unweighted_*_th_*.txt.mcliques\n",
    "do\n",
    "    echo \"Processing ${mcliques}.\"\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "    # changing dir so that output files can be saved under each th dirs.\n",
    "    cd $(dirname ${mcliques})\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "    ${toold}/cos ${mcliques}\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "    # N_communities.txt (1:community_id 2..N: maximal_clique)\n",
    "    # k_num_communities.txt (1:k 2: number of k-clique communities discovered)\n",
    "\n",
    "    echo \"${mcliques} done.\"\n",
    "    [ $(plus $(echo ${PIPESTATUS[@]})) -eq \"0\" ] || ERROR_EXIT\n",
    "done\n",
    "\n",
    "exit 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 6: back_to_org_words.sh\n",
    "This script reverts integers in `N_commnities.txt` to original words using map file generated by `maximal_cliques`.\n",
    "\n",
    "Output: produces `twitter/yyyymmdd/th_XX/namedN_communities.txt`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash -xv\n",
    "\n",
    "# back_to_org_words.sh: \n",
    "# use map file generated by maximal_cliques to revert integers in N_commnities.txt\n",
    "# to original words.\n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "tmp=/tmp/$$\n",
    "\n",
    "# TODO test\n",
    "#datad=${homed}/DATA.mini\n",
    "#workd=${homed}/twitter.mini\n",
    "\n",
    "\n",
    "# error function: show ERROR and delete tmp files\n",
    "ERROR_EXIT() {\n",
    "  echo \"ERROR\"\n",
    "  rm -f $tmp-*\n",
    "  exit 1\n",
    "}\n",
    "\n",
    "\n",
    "echo ${workd}/*/th_*/[0-9]*_communities.txt\t\t|\n",
    "tarr\t\t\t\t\t\t\t|\n",
    "ugrep -v '\\*'\t\t\t\t\t\t|\n",
    "\n",
    "# community番号とthreshold数を変数に入れてwhileループをする必要がある\n",
    "\n",
    "while read community_files; do\n",
    ":>$tmp-tran\n",
    "\n",
    "   echo ${community_files}\n",
    "\n",
    "   # get directory path of target-file \n",
    "   dirname=$(dirname ${community_files})\n",
    "   # get filename\n",
    "   filename=$(basename ${community_files})\n",
    "\n",
    "   # read a community file\n",
    "   fsed 's/:/ /1' ${community_files}                    |\n",
    "   # 1: community id 2..N: integer for node\n",
    "\n",
    "   # remove unnecessary space char at the end of each line\n",
    "   sed -e 's/ *$//'                                     |\n",
    "\n",
    "   # remove lines which have only 1 field for community id\n",
    "   gawk 'NF>1'\t\t\t\t\t\t|\n",
    "\n",
    "   tarr num=1\t\t\t\t\t\t|\n",
    "   # 1: community id 2: integer\n",
    "\n",
    "   self 2 1\t\t\t\t\t\t|\n",
    "   # 1: integer 2: community id\n",
    "   # sort by field 1/2\n",
    "   msort key=1/2                                        |\n",
    "   # remove the same records, only take last one \n",
    "   getlast 1 2                                          > $tmp-tran\n",
    "   # 1: integer 2: community id\n",
    "\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "   # TODO: for debug\n",
    "   cat $tmp-tran\n",
    "\n",
    "   # Read the word-map file\n",
    "   cat ${dirname}/unweighted_*_th_*.txt.map             |\n",
    "   # 1: word 2: integer\n",
    "   self 2 1                                             |\n",
    "   # 1: integer 2: word\n",
    "   # sort by field 1\n",
    "   msort key=1                                          |\n",
    "   # join map file to community -tran\n",
    "   join1 key=1 - $tmp-tran                              |\n",
    "   # 1: integer 2: word 3: community id\n",
    "   self 3 2                                             |\n",
    "   # 1: community id 2: word\n",
    "   yarr num=1 > ${dirname}/named${filename}                          \n",
    "   # 1: community id 2..N: word1..N\n",
    "\n",
    "   [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "done\n",
    "[ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "# delete tmp files\n",
    "rm -f $tmp-*\n",
    "\n",
    "exit 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: compute_transition_likelihoods.sh\n",
    "This script will compute transition-likelihoods map files using `named_N_communities.txt`.\n",
    "    \n",
    "Output: produces `twitter/yyyymmdd/th_XX/namedN_communities_transition.csv`\n",
    "\n",
    "```shell\n",
    "#!/bin/bash -xv\n",
    "\n",
    "# compute_transition_likelihoods.sh\n",
    "#\n",
    "\n",
    "homed=/home/James.P.H/UNICAGE\n",
    "toold=${homed}/TOOL\n",
    "shelld=${homed}/SHELL\n",
    "rawd=/home/James.P.H/data\n",
    "semd=${homed}/SEMAPHORE\n",
    "datad=${homed}/DATA\n",
    "workd=${homed}/twitter\n",
    "\n",
    "tmp=/tmp/$$\n",
    "\n",
    "# TODO test\n",
    "#shelld=${homed}/SHELL/sugi_test\n",
    "#datad=${homed}/DATA.mini\n",
    "#workd=${homed}/twitter.mini\n",
    "\n",
    "# error function: show ERROR and delete tmp files\n",
    "ERROR_EXIT() {\n",
    "  echo \"ERROR\"\n",
    "  rm -f $tmp-*\n",
    "  exit 1\n",
    "}\n",
    "\n",
    "# 対象の日付リストを作成\n",
    "echo ${workd}/2*\t\t\t\t|\n",
    "tarr\t\t\t\t\t\t|\n",
    "ugrep -v '\\*'\t\t\t\t\t|\n",
    "sed -e 's/\\// /g'\t\t\t\t|\n",
    "self NF\t\t\t\t\t\t|\n",
    "msort key=1\t\t\t\t\t> $tmp-date-dir-list\n",
    "# 1:date(real dir)\n",
    "\n",
    "[ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "\n",
    "fromdate=$(head -1 $tmp-date-dir-list)\n",
    "todate=$(tail -1 $tmp-date-dir-list)\n",
    "\n",
    "mdate -e ${fromdate} ${todate}\t\t\t> $tmp-date-list\n",
    "# 1:date\n",
    "\n",
    "\n",
    "for curr_date in $(cat $tmp-date-list); do\n",
    "\n",
    "  next_date=$(mdate ${curr_date}/+1)\n",
    "\n",
    "  n=0\n",
    "  echo ${workd}/${curr_date}/th_*/named*_communities.txt\t|\n",
    "  tarr\t\t\t\t\t\t|\n",
    "  ugrep -v '\\*'\t\t\t\t\t|\n",
    "  while read curr_filename; do\n",
    "    n=$((n+1))\n",
    "    echo ${curr_filename} $n\n",
    "\n",
    "    {\n",
    "      # extract end of filepath (ex: th_02/named3_communities.txt)\n",
    "      tmp_next_filename=$(echo ${curr_filename} | sed -e 's/\\// /g' | self NF-1/NF | sed -e 's/ /\\//g')\n",
    "  \n",
    "      [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "  \n",
    "      # create next_date's filepath\n",
    "      next_filename=${workd}/${next_date}/${tmp_next_filename}\n",
    "  \n",
    "      if [ ! -s ${next_filename} ]; then\n",
    "        touch ${semd}/sem.$n\n",
    "        continue\n",
    "      fi\n",
    "  \n",
    "      # create sets of tag for each community\n",
    "      tarr num=1 ${curr_filename}\t\t|\n",
    "      # 1:community id 2:tag\n",
    "      # exclude dupulicated tag in each community \n",
    "      msort key=1/2\t\t\t\t|\n",
    "      uniq\t\t\t\t\t|\n",
    "      yarr -d, num=1\t\t\t\t> $tmp-curr_cluster.$n\n",
    "      # 1:community id 2:tagset(csv)\n",
    "      #  ex) 0 tag1,tag2,tag3,...\n",
    "      #      1 tag1,tag3,...\n",
    "      [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "  \n",
    "      # create sets of tag for each community\n",
    "      #   same as ${curr_filename}\n",
    "      tarr num=1 ${next_filename}\t\t|\n",
    "      msort key=1/2\t\t\t\t|\n",
    "      uniq\t\t\t\t\t|\n",
    "      yarr -d, num=1\t\t\t\t> $tmp-next_cluster.$n\n",
    "      [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "  \n",
    "      joinx $tmp-curr_cluster.$n $tmp-next_cluster.$n\t> $tmp-joinx_cluster.$n\n",
    "      # 1:id(curr) 2:tagset(curr) 3:id(next) 4:tagset(next)\n",
    "  \n",
    "      self 1 3 2 4 $tmp-joinx_cluster.$n\t> $tmp-joinx_cluster_wk.$n\n",
    "      # 1:id(curr) 2:id(next) 3:tagset(curr) 4:tagset(next)\n",
    "\n",
    "      ${shelld}/intersection.test $tmp-joinx_cluster_wk.$n > $tmp-likelihood.$n\n",
    "      # 1:id(curr) 2:id(next) 3:count\n",
    "   \n",
    "      # create map: index=id(curr) columns=id(next)\n",
    "      map num=1 $tmp-likelihood.$n\t\t> $tmp-likelihood.map.$n\n",
    "  \n",
    "      # csv file name\n",
    "      dirname=$(dirname ${curr_filename})\n",
    "      mkdir -p $dirname\n",
    "      csv_filename=$(basename ${curr_filename} '.txt' | gawk '{ print \"'${dirname}'/\"$0\"_transition.csv\" }')\n",
    "      [ $(plus $(echo \"${PIPESTATUS[@]}\")) -eq \"0\" ] || ERROR_EXIT\n",
    "  \n",
    "      # comvert to csv\n",
    "      tocsv $tmp-likelihood.map.$n\t\t\t> ${csv_filename}\n",
    "\n",
    "      # run 5 processes in parallel\n",
    "      touch ${semd}/sem.$n\n",
    "    } &\n",
    "\n",
    "   if [ $((n % 5)) -eq 0 ]; then\n",
    "     eval semwait ${semd}/sem.{$((n-4))..$n}\n",
    "     eval rm ${semd}/sem.*\n",
    "   fi\n",
    "\n",
    "  done\n",
    "\n",
    "  #semwait \"${semd}/sem.*\"\n",
    "  eval rm ${semd}/sem.*\n",
    "\n",
    "done\n",
    "\n",
    "\n",
    "# delete tmp files\n",
    "rm -f $tmp-*\n",
    "\n",
    "exit 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
